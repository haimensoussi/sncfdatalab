{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a42818a-9cdf-4858-a85c-d086733d008d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Nouveau dataset de novembre : trains additionnels ou mis à jour\n",
    "# -------------------------------------------------------------------------\n",
    "data_nov = \"\"\"TrainID,TrainType,DepartureStation,ArrivalStation,DepartureTime,ArrivalTime,DelayMinutes\n",
    "TGV010,TGV,Paris,Nice,2024-11-02 07:00:00,2024-11-02 11:20:00,0\n",
    "TGV011,TGV,Nice,Paris,2024-11-02 13:00:00,2024-11-02 17:30:00,5\n",
    "TER108,TER,Paris,Lille,2024-11-02 08:30:00,2024-11-02 10:50:00,12\n",
    "TER109,TER,Reims,Paris,2024-11-02 09:15:00,2024-11-02 11:00:00,0\n",
    "IC205,Intercités,Nantes,Bordeaux,2024-11-02 10:00:00,2024-11-02 13:10:00,3\n",
    "IC202,Intercités,Bordeaux,Nantes,2024-11-02 09:00:00,2024-11-02 12:15:00,25  # même ID qu'avant, mais retard mis à jour\n",
    "\"\"\"\n",
    "\n",
    "path_nov = \"/dbfs/spark_lab/train_schedule_2024_11.csv\"\n",
    "dbutils.fs.put(\n",
    "    path_nov,\n",
    "    data_nov,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Dataset créé : {path_nov}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c5500a-4923-4d69-9675-1065f1cae506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lecture du fichier novembre et création du DataFrame df_nov\n",
    "df_nov = spark.read.option(\"header\", True).csv(path_nov)\n",
    "\n",
    "display(df_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7524a5-874d-4b05-94a9-9a1fddb98acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restaure la table Delta à la version précédente (étape n-1)\n",
    "last_version = spark.sql(\"SELECT max(version) as v FROM (DESCRIBE HISTORY sncf_analytics.raw.train_schedule)\").collect()[0]['v']\n",
    "spark.sql(f\"RESTORE TABLE sncf_analytics.raw.train_schedule TO VERSION AS OF {last_version - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d4b20e-7244-495b-961c-35d346f3aef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, date_format, col\n",
    "\n",
    "df_nov = df_nov.withColumn(\n",
    "    \"ServiceUID\",\n",
    "    concat_ws(\"_\", col(\"TrainID\"), date_format(col(\"DepartureTime\"), \"yyyyMMddHHmm\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd253ff7-50f6-4e38-bc23-32402aed5c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.table(\"sncf_analytics.raw.train_schedule\")\n",
    "\n",
    "# Si la clé n’existe pas encore, on la crée\n",
    "if \"ServiceUID\" not in df_raw.columns:\n",
    "    df_raw = df_raw.withColumn(\n",
    "        \"ServiceUID\",\n",
    "        concat_ws(\"_\", col(\"TrainID\"), date_format(col(\"DepartureTime\"), \"yyyyMMddHHmm\"))\n",
    "    )\n",
    "\n",
    "# Réécriture (une seule fois)\n",
    "df_raw.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"sncf_analytics.raw.train_schedule\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc69c58e-b378-4e1e-aeab-9ecabed07d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"sncf_analytics.raw.train_schedule\")\n",
    "\n",
    "(\n",
    "    delta_table.alias(\"t\")\n",
    "    .merge(\n",
    "        df_nov.alias(\"s\"),\n",
    "        \"t.ServiceUID = s.ServiceUID\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()     # met à jour le même trajet (retard corrigé)\n",
    "    .whenNotMatchedInsertAll()  # ajoute les nouveaux voyages\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "print(\"✅ MERGE Delta historisé effectué avec succès (clé ServiceUID utilisée).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311ee5b1-f35d-4c4e-8611-a759d642bf56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT TrainID, DepartureTime, DelayMinutes\n",
    "FROM sncf_analytics.raw.train_schedule\n",
    "ORDER BY TrainID, DepartureTime;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc45963-1f72-413e-b6f5-df916f7b9c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, when, round\n",
    "\n",
    "df = spark.table(\"sncf_analytics.raw.train_schedule\")\n",
    "\n",
    "df_transformed = (\n",
    "    df\n",
    "    .withColumn(\"DepartureTime\", to_timestamp(col(\"DepartureTime\")))\n",
    "    .withColumn(\"ArrivalTime\", to_timestamp(col(\"ArrivalTime\")))\n",
    "    .withColumn(\"DelayMinutes\", col(\"DelayMinutes\").cast(\"int\"))\n",
    "    .withColumn(\"Status\", when(col(\"DelayMinutes\") > 10, \"Late\").otherwise(\"On Time\"))\n",
    "    .withColumn(\"TravelDurationMinutes\",\n",
    "                round((col(\"ArrivalTime\").cast(\"long\") - col(\"DepartureTime\").cast(\"long\")) / 60, 2))\n",
    ")\n",
    "\n",
    "df_transformed.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"sncf_analytics.curated.train_status\")\n",
    "\n",
    "print(\"✅ Couche CURATED mise à jour avec historique et statut actualisé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629e5783-51f6-4ee6-83fe-6dc6e74e8ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  TrainType, \n",
    "  COUNT(*) AS Nb_Voyages,\n",
    "  ROUND(AVG(DelayMinutes),2) AS AvgDelay,\n",
    "  SUM(CASE WHEN Status='Late' THEN 1 ELSE 0 END)*100/COUNT(*) AS LateRate\n",
    "FROM sncf_analytics.curated.train_status\n",
    "GROUP BY TrainType;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7838589875544057,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Data Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
