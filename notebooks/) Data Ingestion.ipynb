{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c20dd2-6a0c-4ee2-b39a-71a01c61e11a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sncf_analytics.raw.salesdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4729e72-2ec9-4009-bc8f-459beb63bf99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "df = spark.table(\"sncf_trafic.raw.salesdata\")\n",
    "\n",
    "df_transformed = (\n",
    "    df\n",
    "    .withColumn(\"DepartureTime\", to_timestamp(col(\"DepartureTime\")))\n",
    "    .withColumn(\"DelayMinutes\", col(\"DelayMinutes\").cast(\"int\"))\n",
    "    .withColumn(\"Status\", when(col(\"DelayMinutes\") > 10, \"Late\").otherwise(\"On Time\"))\n",
    ")\n",
    "\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sncf_trafic.curated.train_status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43e541a3-d8e6-4761-9d09-d6fa231f3e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = \"\"\"TrainID,TrainType,DepartureStation,ArrivalStation,DepartureTime,ArrivalTime,DelayMinutes\n",
    "TGV001,TGV,Paris,Lyon,2024-10-30 06:45:00,2024-10-30 09:05:00,0\n",
    "TGV002,TGV,Paris,Marseille,2024-10-30 07:00:00,2024-10-30 10:00:00,8\n",
    "TGV003,TGV,Lyon,Paris,2024-10-30 07:15:00,2024-10-30 09:35:00,3\n",
    "TGV004,TGV,Marseille,Paris,2024-10-30 08:00:00,2024-10-30 11:00:00,12\n",
    "TGV005,TGV,Paris,Bordeaux,2024-10-30 09:00:00,2024-10-30 11:20:00,0\n",
    "TER101,TER,Lille,Paris,2024-10-30 07:30:00,2024-10-30 09:45:00,15\n",
    "TER102,TER,Reims,Paris,2024-10-30 08:10:00,2024-10-30 09:50:00,2\n",
    "TER103,TER,Paris,Chartres,2024-10-30 10:00:00,2024-10-30 11:05:00,0\n",
    "TER104,TER,Orléans,Paris,2024-10-30 10:30:00,2024-10-30 12:00:00,20\n",
    "TER105,TER,Paris,Lille,2024-10-30 11:00:00,2024-10-30 13:10:00,5\n",
    "IC201,Intercités,Toulouse,Bordeaux,2024-10-30 06:30:00,2024-10-30 08:45:00,0\n",
    "IC202,Intercités,Bordeaux,Nantes,2024-10-30 09:00:00,2024-10-30 12:15:00,18\n",
    "IC203,Intercités,Nantes,Bordeaux,2024-10-30 13:00:00,2024-10-30 16:10:00,10\n",
    "IC204,Intercités,Bordeaux,Toulouse,2024-10-30 16:30:00,2024-10-30 18:45:00,0\n",
    "TGV006,TGV,Paris,Lyon,2024-10-30 12:00:00,2024-10-30 14:20:00,25\n",
    "TGV007,TGV,Lyon,Paris,2024-10-30 15:00:00,2024-10-30 17:15:00,0\n",
    "TER106,TER,Paris,Reims,2024-10-30 16:00:00,2024-10-30 17:30:00,4\n",
    "TER107,TER,Paris,Orléans,2024-10-30 17:30:00,2024-10-30 18:50:00,0\n",
    "TGV008,TGV,Marseille,Lyon,2024-10-30 18:00:00,2024-10-30 19:45:00,10\n",
    "TGV009,TGV,Paris,Strasbourg,2024-10-30 19:00:00,2024-10-30 21:15:00,7\n",
    "\"\"\"\n",
    "\n",
    "path = \"abfss://root@dbstorageqf6i3byf7qjb4.dfs.core.windows.net/3073971834093534/user/hive/warehouse/Dev/Raw/train_schedule.csv\"\n",
    "dbutils.fs.put(\n",
    "    path,\n",
    "    data,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Dataset créé : {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649ecc57-9aee-4bcc-b02e-17322f4c7158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.ls(\"abfss://root@dbstorageqf6i3byf7qjb4.dfs.core.windows.net/3073971834093534/user/hive/warehouse/Dev/Raw/\")\n",
    "    print(\"You have access to this path.\")\n",
    "except Exception as e:\n",
    "    print(f\"No access or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0ac650-9f40-461b-ac5a-83abf01e2af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"abfss://root@dbstorageqf6i3byf7qjb4.dfs.core.windows.net/3073971834093534/user/hive/warehouse/Dev/Raw/train_schedule.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d737df-9a86-4e42-9111-78f6380ca783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = \"\"\"TrainID,TrainType,DepartureStation,ArrivalStation,DepartureTime,ArrivalTime,DelayMinutes\n",
    "TGV001,TGV,Paris,Lyon,2024-10-30 06:45:00,2024-10-30 09:05:00,0\n",
    "TGV002,TGV,Paris,Marseille,2024-10-30 07:00:00,2024-10-30 10:00:00,8\n",
    "TGV003,TGV,Lyon,Paris,2024-10-30 07:15:00,2024-10-30 09:35:00,3\n",
    "TGV004,TGV,Marseille,Paris,2024-10-30 08:00:00,2024-10-30 11:00:00,12\n",
    "TGV005,TGV,Paris,Bordeaux,2024-10-30 09:00:00,2024-10-30 11:20:00,0\n",
    "TER101,TER,Lille,Paris,2024-10-30 07:30:00,2024-10-30 09:45:00,15\n",
    "TER102,TER,Reims,Paris,2024-10-30 08:10:00,2024-10-30 09:50:00,2\n",
    "TER103,TER,Paris,Chartres,2024-10-30 10:00:00,2024-10-30 11:05:00,0\n",
    "TER104,TER,Orléans,Paris,2024-10-30 10:30:00,2024-10-30 12:00:00,20\n",
    "TER105,TER,Paris,Lille,2024-10-30 11:00:00,2024-10-30 13:10:00,5\n",
    "IC201,Intercités,Toulouse,Bordeaux,2024-10-30 06:30:00,2024-10-30 08:45:00,0\n",
    "IC202,Intercités,Bordeaux,Nantes,2024-10-30 09:00:00,2024-10-30 12:15:00,18\n",
    "IC203,Intercités,Nantes,Bordeaux,2024-10-30 13:00:00,2024-10-30 16:10:00,10\n",
    "IC204,Intercités,Bordeaux,Toulouse,2024-10-30 16:30:00,2024-10-30 18:45:00,0\n",
    "TGV006,TGV,Paris,Lyon,2024-10-30 12:00:00,2024-10-30 14:20:00,25\n",
    "TGV007,TGV,Lyon,Paris,2024-10-30 15:00:00,2024-10-30 17:15:00,0\n",
    "TER106,TER,Paris,Reims,2024-10-30 16:00:00,2024-10-30 17:30:00,4\n",
    "TER107,TER,Paris,Orléans,2024-10-30 17:30:00,2024-10-30 18:50:00,0\n",
    "\"\"\"\n",
    "\n",
    "# Création du fichier CSV dans DBFS\n",
    "path = \"/dbfs/lab1/train_schedule.csv\"\n",
    "dbutils.fs.put(\"dbfs:/lab1/train_schedule.csv\", data, overwrite=True)\n",
    "\n",
    "print(f\"✅ Fichier créé : {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a7b098-308e-4e74-9f76-3da500598478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sncf_trafic_dev.raw.train_schedule\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ab5a81-d0ab-4b3a-a934-5f480e11492b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Import des fonctions nécessaires\n",
    "# -------------------------------------------------------------------------\n",
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Lecture de la table brute (RAW Layer)\n",
    "# -------------------------------------------------------------------------\n",
    "df = spark.table(\"sncf_trafic_dev.raw.train_schedule\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Transformation et typage des colonnes temporelles\n",
    "# -------------------------------------------------------------------------\n",
    "df_transformed = ( \n",
    "    df\n",
    "    # Convertit la colonne DepartureTime en type timestamp\n",
    "    .withColumn(\"DepartureTime\", to_timestamp(col(\"DepartureTime\")))\n",
    "    \n",
    "    # Convertit la colonne ArrivalTime en type timestamp\n",
    "    .withColumn(\"ArrivalTime\", to_timestamp(col(\"ArrivalTime\")))\n",
    "\n",
    "    # Convertit DelayMinutes en entier (pour les calculs)\n",
    "    .withColumn(\"DelayMinutes\", col(\"DelayMinutes\").cast(\"int\"))\n",
    "\n",
    "    # Crée une nouvelle colonne \"Status\" selon le retard\n",
    "    .withColumn(\"Status\", when(col(\"DelayMinutes\") > 10, \"Late\").otherwise(\"On Time\"))\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Écriture des données transformées dans la couche CURATED (Delta Lake)\n",
    "# -------------------------------------------------------------------------\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sncf_trafic_dev.curated.train_status\")\n",
    "\n",
    "display(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b07bfa-5b13-45fa-9d1f-a958429b08a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Liste des tables disponibles\n",
    "SHOW TABLES IN sncf_analytics.curated;\n",
    "\n",
    "-- Vérifier le contenu\n",
    "SELECT TrainID, TrainType, DelayMinutes, Status FROM sncf_trafic_dev.curated.train_status;\n",
    "\n",
    "-- Comptage et analyse de ponctualité\n",
    "SELECT Status, COUNT(*) AS Nb_Trains\n",
    "FROM sncf_trafic_dev.curated.train_status\n",
    "GROUP BY Status;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7693d6-3cf1-467c-b864-1271aff6e419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import des fonctions nécessaires\n",
    "# -------------------------------------------------------------------------\n",
    "from pyspark.sql.functions import col, to_timestamp, when, round\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Lecture de la table RAW (importée depuis le CSV)\n",
    "# -------------------------------------------------------------------------\n",
    "df = spark.table(\"sncf_trafic_dev.raw.train_schedule\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Transformation et enrichissement\n",
    "# -------------------------------------------------------------------------\n",
    "df_transformed = (\n",
    "    df\n",
    "    # Typage des colonnes temporelles\n",
    "    .withColumn(\"DepartureTime\", to_timestamp(col(\"DepartureTime\")))\n",
    "    .withColumn(\"ArrivalTime\", to_timestamp(col(\"ArrivalTime\")))\n",
    "    .withColumn(\"DelayMinutes\", col(\"DelayMinutes\").cast(\"int\"))\n",
    "\n",
    "    # Calcul du statut de ponctualité\n",
    "    .withColumn(\"Status\", when(col(\"DelayMinutes\") > 10, \"Late\").otherwise(\"On Time\"))\n",
    "\n",
    "    # Calcul de la durée de trajet (ArrivalTime - DepartureTime)\n",
    "    .withColumn(\"TravelDurationMinutes\",\n",
    "                round((col(\"ArrivalTime\").cast(\"long\") - col(\"DepartureTime\").cast(\"long\")) / 60, 2))\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Écriture dans la table CURATED (Delta Lake + Unity Catalog)\n",
    "# -------------------------------------------------------------------------\n",
    "df_transformed.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"sncf_trafic_dev.curated.train_status\")\n",
    "\n",
    "print(\"✅ Table sncf_analytics.curated.train_status mise à jour avec la durée du trajet (TravelDurationMinutes).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a02afe-291c-4100-ac55-0ae89a418bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM sncf_trafic_dev.curated.train_status LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52839a7-7bc0-4cf4-a7d7-04bf0782af88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT TrainType,\n       ROUND(AVG(DelayMinutes),2) AS AvgDelay,\n       ROUND(AVG(TravelDurationMinutes),2) AS AvgDuration\nFROM sncf_analytics.curated.train_status\nGROUP BY TrainType) SELECT `TrainType`,SUM(`AvgDelay`) `column_708f50a2100`,`TrainType` FROM q GROUP BY `TrainType`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "TrainType",
             "id": "column_708f50a2101"
            },
            "x": {
             "column": "TrainType",
             "id": "column_708f50a2103"
            },
            "y": [
             {
              "column": "AvgDelay",
              "id": "column_708f50a2100",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_708f50a2100": {
             "name": "AvgDuration",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "ba61b16c-c64e-45a4-9938-4b9377da4f2c",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 17.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "TrainType",
           "type": "column"
          },
          {
           "column": "TrainType",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "TrainType",
           "type": "column"
          },
          {
           "alias": "column_708f50a2100",
           "args": [
            {
             "column": "AvgDelay",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "TrainType",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sncf_trafic_dev.raw.train_schedulesncf_trafic_dev.raw.train_schedule%sql\n",
    "-- Lister les tables disponibles dans la couche CURATED\n",
    "SHOW TABLES IN sncf_analytics.curated;\n",
    "\n",
    "-- Vérifier le contenu principal\n",
    "SELECT TrainID, TrainType, DepartureStation, ArrivalStation,\n",
    "       DepartureTime, ArrivalTime, DelayMinutes, Status, TravelDurationMinutes\n",
    "FROM sncf_trafic_dev.curated.train_status\n",
    "ORDER BY DepartureTime;\n",
    "\n",
    "-- Vérifier les moyennes de durée et retards\n",
    "SELECT TrainType,\n",
    "       ROUND(AVG(DelayMinutes),2) AS AvgDelay,\n",
    "       ROUND(AVG(TravelDurationMinutes),2) AS AvgDuration\n",
    "FROM sncf_trafic_dev.curated.train_status\n",
    "GROUP BY TrainType;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f645db1b-9232-49e3-8ffc-0c997038fef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"sncf_trafic_dev.curated.train_status\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da534915-0ae9-4a5b-ad4d-ed3882638426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, max, min, count, sum\n",
    "\n",
    "# Moyenne du retard par type de train\n",
    "agg_avg_delay = df.groupBy(\"TrainType\").agg(avg(\"DelayMinutes\").alias(\"AvgDelay\"))\n",
    "display(agg_avg_delay)\n",
    "\n",
    "# Nombre de trajets par station de départ\n",
    "agg_count_departure = df.groupBy(\"DepartureStation\").agg(count(\"*\").alias(\"NbTrajets\"))\n",
    "display(agg_count_departure)\n",
    "\n",
    "# Retard maximum par type de train\n",
    "agg_max_delay = df.groupBy(\"TrainType\").agg(max(\"DelayMinutes\").alias(\"MaxDelay\"))\n",
    "display(agg_max_delay)\n",
    "\n",
    "# Somme totale des retards par station d'arrivée\n",
    "agg_sum_delay = df.groupBy(\"ArrivalStation\").agg(sum(\"DelayMinutes\").alias(\"TotalDelay\"))\n",
    "display(agg_sum_delay)\n",
    "\n",
    "# Durée de trajet minimale et maximale par type de train (si TravelDurationMinutes existe)\n",
    "if \"TravelDurationMinutes\" in df.columns:\n",
    "    agg_duration = df.groupBy(\"TrainType\").agg(\n",
    "        min(\"TravelDurationMinutes\").alias(\"MinDuration\"),\n",
    "        max(\"TravelDurationMinutes\").alias(\"MaxDuration\")\n",
    "    )\n",
    "    display(agg_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ec6fa1-0467-435a-903e-2aa4c113025c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBWaXN1YWxpc2F0aW9uIDEgOiBSw6lwYXJ0aXRpb24gZGVzIHN0YXR1dHMgZGUgcG9uY3R1YWxpdMOpCmRmID0gc3BhcmsudGFibGUoInNuY2ZfdHJhZmljX2Rldi5jdXJhdGVkLnRyYWluX3N0YXR1cyIpCmRpc3BsYXkoZGYuZ3JvdXBCeSgiU3RhdHVzIikuY291bnQoKSkKCiMgVmlzdWFsaXNhdGlvbiAyIDogTW95ZW5uZSBkdSByZXRhcmQgcGFyIHR5cGUgZGUgdHJhaW4KZGlzcGxheShkZi5ncm91cEJ5KCJUcmFpblR5cGUiKS5hdmcoIkRlbGF5TWludXRlcyIpKQoKIyBWaXN1YWxpc2F0aW9uIDMgOiBEdXLDqWUgbW95ZW5uZSBkZSB0cmFqZXQgcGFyIHR5cGUgZGUgdHJhaW4KaWYgIlRyYXZlbER1cmF0aW9uTWludXRlcyIgaW4gZGYuY29sdW1uczoKICAgIGRpc3BsYXkoZGYuZ3JvdXBCeSgiVHJhaW5UeXBlIikuYXZnKCJUcmF2ZWxEdXJhdGlvbk1pbnV0ZXMiKSkKCiMgVmlzdWFsaXNhdGlvbiA0IDogTm9tYnJlIGRlIHRyYWpldHMgcGFyIHN0YXRpb24gZGUgZMOpcGFydApkaXNwbGF5KGRmLmdyb3VwQnkoIkRlcGFydHVyZVN0YXRpb24iKS5jb3VudCgpKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 3:\n        # create a temp view\n        if type(__backend_agg_dfs[3]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[3].to_spark().createOrReplaceTempView(\"DatabricksView7645b1b\")\n        elif type(__backend_agg_dfs[3]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[3], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[3]).createOrReplaceTempView(\"DatabricksView7645b1b\")\n        else:\n            __backend_agg_dfs[3].createOrReplaceTempView(\"DatabricksView7645b1b\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView7645b1b) SELECT `DepartureStation`,SUM(`count`) `column_980da43154` FROM q GROUP BY `DepartureStation`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView7645b1b\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "DepartureStation",
             "id": "column_980da43153"
            },
            "y": [
             {
              "column": "count",
              "id": "column_980da43154",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_980da43154": {
             "name": "count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "8322f7d0-4f2c-43ed-9d79-1156d3d4c34f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 16.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "DepartureStation",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "DepartureStation",
           "type": "column"
          },
          {
           "alias": "column_980da43154",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 3,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBWaXN1YWxpc2F0aW9uIDEgOiBSw6lwYXJ0aXRpb24gZGVzIHN0YXR1dHMgZGUgcG9uY3R1YWxpdMOpCmRmID0gc3BhcmsudGFibGUoInNuY2ZfdHJhZmljX2Rldi5jdXJhdGVkLnRyYWluX3N0YXR1cyIpCmRpc3BsYXkoZGYuZ3JvdXBCeSgiU3RhdHVzIikuY291bnQoKSkKCiMgVmlzdWFsaXNhdGlvbiAyIDogTW95ZW5uZSBkdSByZXRhcmQgcGFyIHR5cGUgZGUgdHJhaW4KZGlzcGxheShkZi5ncm91cEJ5KCJUcmFpblR5cGUiKS5hdmcoIkRlbGF5TWludXRlcyIpKQoKIyBWaXN1YWxpc2F0aW9uIDMgOiBEdXLDqWUgbW95ZW5uZSBkZSB0cmFqZXQgcGFyIHR5cGUgZGUgdHJhaW4KaWYgIlRyYXZlbER1cmF0aW9uTWludXRlcyIgaW4gZGYuY29sdW1uczoKICAgIGRpc3BsYXkoZGYuZ3JvdXBCeSgiVHJhaW5UeXBlIikuYXZnKCJUcmF2ZWxEdXJhdGlvbk1pbnV0ZXMiKSkKCiMgVmlzdWFsaXNhdGlvbiA0IDogTm9tYnJlIGRlIHRyYWpldHMgcGFyIHN0YXRpb24gZGUgZMOpcGFydApkaXNwbGF5KGRmLmdyb3VwQnkoIkRlcGFydHVyZVN0YXRpb24iKS5jb3VudCgpKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 2:\n        # create a temp view\n        if type(__backend_agg_dfs[2]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[2].to_spark().createOrReplaceTempView(\"DatabricksView4d526c8\")\n        elif type(__backend_agg_dfs[2]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[2], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[2]).createOrReplaceTempView(\"DatabricksView4d526c8\")\n        else:\n            __backend_agg_dfs[2].createOrReplaceTempView(\"DatabricksView4d526c8\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView4d526c8) SELECT `TrainType`,SUM(`avg(TravelDurationMinutes)`) `column_980da43157` FROM q GROUP BY `TrainType`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView4d526c8\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "TrainType",
             "id": "column_980da43156"
            },
            "y": [
             {
              "column": "avg(TravelDurationMinutes)",
              "id": "column_980da43157",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_980da43157": {
             "name": "avg(TravelDurationMinutes)",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "4c9f5da7-0b7c-4483-97d7-05f9040210a4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 16.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "TrainType",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "TrainType",
           "type": "column"
          },
          {
           "alias": "column_980da43157",
           "args": [
            {
             "column": "avg(TravelDurationMinutes)",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 2,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBWaXN1YWxpc2F0aW9uIDEgOiBSw6lwYXJ0aXRpb24gZGVzIHN0YXR1dHMgZGUgcG9uY3R1YWxpdMOpCmRmID0gc3BhcmsudGFibGUoInNuY2ZfdHJhZmljX2Rldi5jdXJhdGVkLnRyYWluX3N0YXR1cyIpCmRpc3BsYXkoZGYuZ3JvdXBCeSgiU3RhdHVzIikuY291bnQoKSkKCiMgVmlzdWFsaXNhdGlvbiAyIDogTW95ZW5uZSBkdSByZXRhcmQgcGFyIHR5cGUgZGUgdHJhaW4KZGlzcGxheShkZi5ncm91cEJ5KCJUcmFpblR5cGUiKS5hdmcoIkRlbGF5TWludXRlcyIpKQoKIyBWaXN1YWxpc2F0aW9uIDMgOiBEdXLDqWUgbW95ZW5uZSBkZSB0cmFqZXQgcGFyIHR5cGUgZGUgdHJhaW4KaWYgIlRyYXZlbER1cmF0aW9uTWludXRlcyIgaW4gZGYuY29sdW1uczoKICAgIGRpc3BsYXkoZGYuZ3JvdXBCeSgiVHJhaW5UeXBlIikuYXZnKCJUcmF2ZWxEdXJhdGlvbk1pbnV0ZXMiKSkKCiMgVmlzdWFsaXNhdGlvbiA0IDogTm9tYnJlIGRlIHRyYWpldHMgcGFyIHN0YXRpb24gZGUgZMOpcGFydApkaXNwbGF5KGRmLmdyb3VwQnkoIkRlcGFydHVyZVN0YXRpb24iKS5jb3VudCgpKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 1:\n        # create a temp view\n        if type(__backend_agg_dfs[1]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[1].to_spark().createOrReplaceTempView(\"DatabricksView13efa8e\")\n        elif type(__backend_agg_dfs[1]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[1], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[1]).createOrReplaceTempView(\"DatabricksView13efa8e\")\n        else:\n            __backend_agg_dfs[1].createOrReplaceTempView(\"DatabricksView13efa8e\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView13efa8e) SELECT `TrainType`,SUM(`avg(DelayMinutes)`) `column_980da43160` FROM q GROUP BY `TrainType`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView13efa8e\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "TrainType",
             "id": "column_980da43159"
            },
            "y": [
             {
              "column": "avg(DelayMinutes)",
              "id": "column_980da43160",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_980da43160": {
             "name": "avg(DelayMinutes)",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "9d32fe4e-823f-4206-8b9f-e46754a46296",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 16.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "TrainType",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "TrainType",
           "type": "column"
          },
          {
           "alias": "column_980da43160",
           "args": [
            {
             "column": "avg(DelayMinutes)",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 1,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation 1 : Répartition des statuts de ponctualité\n",
    "df = spark.table(\"sncf_trafic_dev.curated.train_status\")\n",
    "display(df.groupBy(\"Status\").count())\n",
    "\n",
    "# Visualisation 2 : Moyenne du retard par type de train\n",
    "display(df.groupBy(\"TrainType\").avg(\"DelayMinutes\"))\n",
    "\n",
    "# Visualisation 3 : Durée moyenne de trajet par type de train\n",
    "if \"TravelDurationMinutes\" in df.columns:\n",
    "    display(df.groupBy(\"TrainType\").avg(\"TravelDurationMinutes\"))\n",
    "\n",
    "# Visualisation 4 : Nombre de trajets par station de départ\n",
    "display(df.groupBy(\"DepartureStation\").count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5384711106652456,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": ") Data Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
